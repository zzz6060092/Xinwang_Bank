{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:55:12.324884Z",
     "start_time": "2020-11-03T05:55:12.312884Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:55:13.486897Z",
     "start_time": "2020-11-03T05:55:13.479911Z"
    }
   },
   "outputs": [],
   "source": [
    "v = 2\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:55:14.462875Z",
     "start_time": "2020-11-03T05:55:14.351869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21047 entries, 0 to 21046\n",
      "Columns: 266 entries, id to target\n",
      "dtypes: float32(252), float64(6), int64(8)\n",
      "memory usage: 22.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_feature = pd.read_pickle('feature{}.pkl'.format(v))\n",
    "df_feature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:55:16.603834Z",
     "start_time": "2020-11-03T05:55:16.504829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15280, 266), (5767, 266))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_feature[df_feature.target.notna()].copy()\n",
    "df_test = df_feature[df_feature.target.isna()].copy()\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:55:20.376747Z",
     "start_time": "2020-11-03T05:55:20.367747Z"
    }
   },
   "outputs": [],
   "source": [
    "ycol = 'target'\n",
    "feature_names = list(\n",
    "    filter(lambda x: x not in [ycol, 'id'], df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:56:28.637113Z",
     "start_time": "2020-11-03T05:55:34.273611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting Model\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's auc: 0.847781\tvalid_0's binary_logloss: 0.256598\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's auc: 0.836879\tvalid_0's binary_logloss: 0.260394\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's auc: 0.839056\tvalid_0's binary_logloss: 0.258275\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's auc: 0.846509\tvalid_0's binary_logloss: 0.256794\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's auc: 0.831352\tvalid_0's binary_logloss: 0.261737\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid_0's auc: 0.841547\tvalid_0's binary_logloss: 0.25905\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's auc: 0.839962\tvalid_0's binary_logloss: 0.260222\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's auc: 0.842971\tvalid_0's binary_logloss: 0.256398\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's auc: 0.850615\tvalid_0's binary_logloss: 0.254868\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's auc: 0.836853\tvalid_0's binary_logloss: 0.262787\n",
      "\n",
      "87 features with zero importance after one-hot encoding.\n",
      "\n",
      "125 features required for cumulative importance of 0.97 after one hot encoding.\n",
      "139 features do not contribute to cumulative importance of 0.97.\n",
      "\n",
      "====low_importance_features=====\n",
      "['x_num_70_squence_emb_2', 'x_num_69_squence_emb_1', 'x_num_17_squence_emb_2', 'x_num_6_squence_emb_3', 'x_num_23_squence_emb_0', 'x_num_6_squence_emb_1', 'x_num_51_squence_emb_2', 'x_cat_9', 'x_num_9_squence_emb_1', 'x_cat_7', 'x_num_60_squence_emb_2', 'x_num_60_squence_emb_0', 'x_num_37_squence_emb_1', 'x_num_60_squence_emb_3', 'x_num_9_squence_emb_3', 'x_num_17_squence_emb_0', 'x_num_37_squence_emb_0', 'x_num_9_squence_emb_2', 'x_num_37_squence_emb_3', 'x_num_23_squence_emb_1', 'x_num_6_squence_emb_2', 'x_num_60_squence_emb_1', 'x_num_29_squence_emb_2', 'x_cat_3', 'x_num_48_squence_emb_0', 'x_num_31_squence_emb_0', 'x_num_34_squence_emb_0', 'x_num_11_squence_emb_1', 'x_num_54_squence_emb_0', 'x_num_7_squence_emb_0', 'x_num_16_squence_emb_0', 'x_num_21_squence_emb_1', 'x_num_44_squence_emb_0', 'x_num_36_squence_emb_3', 'x_num_56_squence_emb_2', 'x_num_12_squence_emb_0', 'x_num_7_squence_emb_1', 'x_num_16_squence_emb_3', 'x_num_65_squence_emb_0', 'x_num_61_squence_emb_2', 'x_num_12_squence_emb_2', 'x_num_54_squence_emb_3', 'x_num_61_squence_emb_0', 'x_num_61_squence_emb_3', 'x_num_21_squence_emb_0', 'x_num_44_squence_emb_2', 'x_num_34_squence_emb_2', 'x_num_42_squence_emb_3', 'x_num_35_squence_emb_1', 'x_num_41_squence_emb_3', 'x_num_67_squence_emb_0', 'x_num_49_squence_emb_0', 'x_num_31_squence_emb_2', 'x_num_48_squence_emb_1', 'x_num_16_squence_emb_1', 'x_num_16_squence_emb_2', 'x_num_31_squence_emb_1', 'x_num_29_squence_emb_3', 'x_num_29_squence_emb_0', 'x_num_34_squence_emb_1', 'x_num_57_squence_emb_0', 'x_num_34_squence_emb_3', 'x_num_35_squence_emb_2', 'x_num_35_squence_emb_3', 'x_num_36_squence_emb_0', 'x_num_36_squence_emb_1', 'x_num_36_squence_emb_2', 'x_num_40_squence_emb_0', 'x_num_40_squence_emb_1', 'x_num_40_squence_emb_2', 'x_num_35_squence_emb_0', 'x_num_40_squence_emb_3', 'x_num_56_squence_emb_3', 'x_num_56_squence_emb_0', 'x_num_49_squence_emb_1', 'x_num_49_squence_emb_2', 'x_num_49_squence_emb_3', 'x_num_12_squence_emb_3', 'x_num_12_squence_emb_1', 'x_num_11_squence_emb_3', 'x_num_52_squence_emb_0', 'x_num_52_squence_emb_1', 'x_num_56_squence_emb_1', 'x_num_52_squence_emb_2', 'x_num_52_squence_emb_3', 'x_num_53_squence_emb_1', 'x_num_53_squence_emb_2', 'x_num_53_squence_emb_3', 'x_num_54_squence_emb_1', 'x_num_54_squence_emb_2', 'x_num_11_squence_emb_2', 'x_num_11_squence_emb_0', 'x_num_48_squence_emb_2', 'x_num_41_squence_emb_0', 'x_num_66_squence_emb_1', 'x_num_41_squence_emb_2', 'x_num_66_squence_emb_2', 'x_num_66_squence_emb_3', 'x_num_64_squence_emb_1', 'x_num_61_squence_emb_1', 'x_num_64_squence_emb_2', 'x_num_67_squence_emb_1', 'x_num_67_squence_emb_2', 'x_num_67_squence_emb_3', 'x_num_62_squence_emb_0', 'x_num_7_squence_emb_3', 'x_num_59_squence_emb_3', 'x_num_58_squence_emb_1', 'x_num_58_squence_emb_2', 'x_num_58_squence_emb_3', 'x_num_59_squence_emb_2', 'x_num_59_squence_emb_1', 'x_num_59_squence_emb_0', 'x_num_29_squence_emb_1', 'x_num_7_squence_emb_2', 'x_num_62_squence_emb_1', 'x_num_62_squence_emb_2', 'x_num_48_squence_emb_3', 'x_num_42_squence_emb_0', 'x_num_42_squence_emb_1', 'x_num_42_squence_emb_2', 'x_num_44_squence_emb_1', 'x_num_44_squence_emb_3', 'x_num_21_squence_emb_3', 'x_num_21_squence_emb_2', 'x_num_57_squence_emb_1', 'x_num_57_squence_emb_2', 'x_num_31_squence_emb_3', 'x_num_58_squence_emb_0', 'x_num_64_squence_emb_0', 'x_num_64_squence_emb_3', 'x_num_65_squence_emb_1', 'x_num_57_squence_emb_3', 'x_num_65_squence_emb_2', 'x_num_65_squence_emb_3', 'x_num_66_squence_emb_0', 'x_num_62_squence_emb_3', 'x_num_41_squence_emb_1', 'x_num_53_squence_emb_0']\n"
     ]
    }
   ],
   "source": [
    "from feature_selector import FeatureSelector\n",
    "fs = FeatureSelector(data = df_train[feature_names], labels = df_train[ycol])\n",
    "fs.identify_zero_importance(task = 'classification', eval_metric = 'auc',\n",
    "                            n_iterations = 10, early_stopping = True)\n",
    "fs.identify_low_importance(cumulative_importance = 0.97)\n",
    "low_importance_features = fs.ops['low_importance']\n",
    "print('====low_importance_features=====')\n",
    "print(low_importance_features)\n",
    "for i in low_importance_features:\n",
    "    feature_names.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:56:43.238914Z",
     "start_time": "2020-11-03T05:56:43.228890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5767, 266)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T16:46:36.255553Z",
     "start_time": "2020-11-03T16:46:36.241552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15280, 266)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T04:20:44.577503Z",
     "start_time": "2020-11-04T03:05:22.740734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "Training until validation scores don't improve for 5000 rounds\n",
      "[100]\ttrain's binary_logloss: 0.360504\ttrain's f1: 0.415108\tvalid's binary_logloss: 0.37724\tvalid's f1: 0.355505\n",
      "[200]\ttrain's binary_logloss: 0.4013\ttrain's f1: 0.44836\tvalid's binary_logloss: 0.427974\tvalid's f1: 0.37617\n",
      "[300]\ttrain's binary_logloss: 0.408838\ttrain's f1: 0.44771\tvalid's binary_logloss: 0.444871\tvalid's f1: 0.355346\n",
      "[400]\ttrain's binary_logloss: 0.400678\ttrain's f1: 0.457917\tvalid's binary_logloss: 0.443614\tvalid's f1: 0.351957\n",
      "[500]\ttrain's binary_logloss: 0.386569\ttrain's f1: 0.473765\tvalid's binary_logloss: 0.436142\tvalid's f1: 0.361328\n",
      "[600]\ttrain's binary_logloss: 0.372114\ttrain's f1: 0.491803\tvalid's binary_logloss: 0.42815\tvalid's f1: 0.365526\n",
      "[700]\ttrain's binary_logloss: 0.356619\ttrain's f1: 0.511308\tvalid's binary_logloss: 0.419096\tvalid's f1: 0.373057\n",
      "[800]\ttrain's binary_logloss: 0.341924\ttrain's f1: 0.530131\tvalid's binary_logloss: 0.410418\tvalid's f1: 0.371945\n",
      "[900]\ttrain's binary_logloss: 0.326849\ttrain's f1: 0.54903\tvalid's binary_logloss: 0.401936\tvalid's f1: 0.377222\n",
      "[1000]\ttrain's binary_logloss: 0.313318\ttrain's f1: 0.564624\tvalid's binary_logloss: 0.393809\tvalid's f1: 0.379557\n",
      "[1100]\ttrain's binary_logloss: 0.299927\ttrain's f1: 0.580545\tvalid's binary_logloss: 0.386474\tvalid's f1: 0.380084\n",
      "[1200]\ttrain's binary_logloss: 0.287882\ttrain's f1: 0.596305\tvalid's binary_logloss: 0.37983\tvalid's f1: 0.384318\n",
      "[1300]\ttrain's binary_logloss: 0.276276\ttrain's f1: 0.609907\tvalid's binary_logloss: 0.373206\tvalid's f1: 0.387828\n",
      "[1400]\ttrain's binary_logloss: 0.26566\ttrain's f1: 0.626639\tvalid's binary_logloss: 0.367899\tvalid's f1: 0.392276\n",
      "[1500]\ttrain's binary_logloss: 0.255858\ttrain's f1: 0.643318\tvalid's binary_logloss: 0.362796\tvalid's f1: 0.391178\n",
      "[1600]\ttrain's binary_logloss: 0.246131\ttrain's f1: 0.656535\tvalid's binary_logloss: 0.357759\tvalid's f1: 0.394016\n",
      "[1700]\ttrain's binary_logloss: 0.237257\ttrain's f1: 0.672788\tvalid's binary_logloss: 0.353163\tvalid's f1: 0.397849\n",
      "[1800]\ttrain's binary_logloss: 0.228897\ttrain's f1: 0.685033\tvalid's binary_logloss: 0.348909\tvalid's f1: 0.399649\n",
      "[1900]\ttrain's binary_logloss: 0.220931\ttrain's f1: 0.70147\tvalid's binary_logloss: 0.345018\tvalid's f1: 0.406069\n",
      "[2000]\ttrain's binary_logloss: 0.213342\ttrain's f1: 0.714993\tvalid's binary_logloss: 0.341443\tvalid's f1: 0.407003\n",
      "[2100]\ttrain's binary_logloss: 0.20622\ttrain's f1: 0.729202\tvalid's binary_logloss: 0.338008\tvalid's f1: 0.41261\n",
      "[2200]\ttrain's binary_logloss: 0.199451\ttrain's f1: 0.740424\tvalid's binary_logloss: 0.33486\tvalid's f1: 0.414909\n",
      "[2300]\ttrain's binary_logloss: 0.19317\ttrain's f1: 0.750173\tvalid's binary_logloss: 0.332175\tvalid's f1: 0.41247\n",
      "[2400]\ttrain's binary_logloss: 0.186879\ttrain's f1: 0.758584\tvalid's binary_logloss: 0.329286\tvalid's f1: 0.41728\n",
      "[2500]\ttrain's binary_logloss: 0.181069\ttrain's f1: 0.768003\tvalid's binary_logloss: 0.326745\tvalid's f1: 0.419162\n",
      "[2600]\ttrain's binary_logloss: 0.175357\ttrain's f1: 0.780461\tvalid's binary_logloss: 0.32431\tvalid's f1: 0.419207\n",
      "[2700]\ttrain's binary_logloss: 0.170016\ttrain's f1: 0.79246\tvalid's binary_logloss: 0.322167\tvalid's f1: 0.423729\n",
      "[2800]\ttrain's binary_logloss: 0.164923\ttrain's f1: 0.803936\tvalid's binary_logloss: 0.320275\tvalid's f1: 0.423817\n",
      "[2900]\ttrain's binary_logloss: 0.160127\ttrain's f1: 0.812383\tvalid's binary_logloss: 0.318418\tvalid's f1: 0.425197\n",
      "[3000]\ttrain's binary_logloss: 0.155466\ttrain's f1: 0.823194\tvalid's binary_logloss: 0.316792\tvalid's f1: 0.419321\n",
      "[3100]\ttrain's binary_logloss: 0.150982\ttrain's f1: 0.833333\tvalid's binary_logloss: 0.31491\tvalid's f1: 0.422043\n",
      "[3200]\ttrain's binary_logloss: 0.146764\ttrain's f1: 0.841757\tvalid's binary_logloss: 0.313173\tvalid's f1: 0.418015\n",
      "[3300]\ttrain's binary_logloss: 0.142789\ttrain's f1: 0.848022\tvalid's binary_logloss: 0.311722\tvalid's f1: 0.412595\n",
      "[3400]\ttrain's binary_logloss: 0.138923\ttrain's f1: 0.85607\tvalid's binary_logloss: 0.310289\tvalid's f1: 0.416667\n",
      "[3500]\ttrain's binary_logloss: 0.13515\ttrain's f1: 0.86704\tvalid's binary_logloss: 0.309058\tvalid's f1: 0.415969\n",
      "[3600]\ttrain's binary_logloss: 0.131574\ttrain's f1: 0.874041\tvalid's binary_logloss: 0.307864\tvalid's f1: 0.415959\n",
      "[3700]\ttrain's binary_logloss: 0.128062\ttrain's f1: 0.879724\tvalid's binary_logloss: 0.306691\tvalid's f1: 0.418099\n",
      "[3800]\ttrain's binary_logloss: 0.124849\ttrain's f1: 0.885119\tvalid's binary_logloss: 0.305894\tvalid's f1: 0.414508\n",
      "[3900]\ttrain's binary_logloss: 0.121694\ttrain's f1: 0.889482\tvalid's binary_logloss: 0.30484\tvalid's f1: 0.414508\n",
      "[4000]\ttrain's binary_logloss: 0.118664\ttrain's f1: 0.896852\tvalid's binary_logloss: 0.303928\tvalid's f1: 0.416667\n",
      "[4100]\ttrain's binary_logloss: 0.115796\ttrain's f1: 0.902836\tvalid's binary_logloss: 0.303176\tvalid's f1: 0.413773\n",
      "[4200]\ttrain's binary_logloss: 0.113098\ttrain's f1: 0.908519\tvalid's binary_logloss: 0.302389\tvalid's f1: 0.410839\n",
      "[4300]\ttrain's binary_logloss: 0.110565\ttrain's f1: 0.913117\tvalid's binary_logloss: 0.301509\tvalid's f1: 0.410798\n",
      "[4400]\ttrain's binary_logloss: 0.107941\ttrain's f1: 0.916984\tvalid's binary_logloss: 0.300751\tvalid's f1: 0.407011\n",
      "[4500]\ttrain's binary_logloss: 0.105546\ttrain's f1: 0.919711\tvalid's binary_logloss: 0.300168\tvalid's f1: 0.4092\n",
      "[4600]\ttrain's binary_logloss: 0.103212\ttrain's f1: 0.923635\tvalid's binary_logloss: 0.299564\tvalid's f1: 0.405405\n",
      "[4700]\ttrain's binary_logloss: 0.100978\ttrain's f1: 0.927195\tvalid's binary_logloss: 0.299167\tvalid's f1: 0.409836\n",
      "[4800]\ttrain's binary_logloss: 0.0988189\ttrain's f1: 0.931183\tvalid's binary_logloss: 0.298686\tvalid's f1: 0.409836\n",
      "[4900]\ttrain's binary_logloss: 0.0968396\ttrain's f1: 0.937229\tvalid's binary_logloss: 0.298247\tvalid's f1: 0.410584\n",
      "[5000]\ttrain's binary_logloss: 0.0950295\ttrain's f1: 0.939262\tvalid's binary_logloss: 0.29797\tvalid's f1: 0.410539\n",
      "[5100]\ttrain's binary_logloss: 0.0932327\ttrain's f1: 0.942944\tvalid's binary_logloss: 0.297686\tvalid's f1: 0.407407\n",
      "[5200]\ttrain's binary_logloss: 0.091584\ttrain's f1: 0.946241\tvalid's binary_logloss: 0.297537\tvalid's f1: 0.403476\n",
      "[5300]\ttrain's binary_logloss: 0.0898938\ttrain's f1: 0.950395\tvalid's binary_logloss: 0.297205\tvalid's f1: 0.401869\n",
      "[5400]\ttrain's binary_logloss: 0.0881604\ttrain's f1: 0.952905\tvalid's binary_logloss: 0.296909\tvalid's f1: 0.398754\n",
      "[5500]\ttrain's binary_logloss: 0.0866031\ttrain's f1: 0.953744\tvalid's binary_logloss: 0.296727\tvalid's f1: 0.397123\n",
      "[5600]\ttrain's binary_logloss: 0.0851776\ttrain's f1: 0.955007\tvalid's binary_logloss: 0.296489\tvalid's f1: 0.396226\n",
      "[5700]\ttrain's binary_logloss: 0.0838369\ttrain's f1: 0.957118\tvalid's binary_logloss: 0.296314\tvalid's f1: 0.39924\n",
      "[5800]\ttrain's binary_logloss: 0.0825616\ttrain's f1: 0.959238\tvalid's binary_logloss: 0.296108\tvalid's f1: 0.396825\n",
      "[5900]\ttrain's binary_logloss: 0.0812752\ttrain's f1: 0.959663\tvalid's binary_logloss: 0.296055\tvalid's f1: 0.396825\n",
      "[6000]\ttrain's binary_logloss: 0.0801146\ttrain's f1: 0.960941\tvalid's binary_logloss: 0.295976\tvalid's f1: 0.396825\n",
      "[6100]\ttrain's binary_logloss: 0.0789931\ttrain's f1: 0.962222\tvalid's binary_logloss: 0.295866\tvalid's f1: 0.398343\n",
      "[6200]\ttrain's binary_logloss: 0.077923\ttrain's f1: 0.963507\tvalid's binary_logloss: 0.295769\tvalid's f1: 0.396673\n",
      "[6300]\ttrain's binary_logloss: 0.0768534\ttrain's f1: 0.964795\tvalid's binary_logloss: 0.295575\tvalid's f1: 0.392535\n",
      "[6400]\ttrain's binary_logloss: 0.0759027\ttrain's f1: 0.966087\tvalid's binary_logloss: 0.295513\tvalid's f1: 0.393295\n",
      "[6500]\ttrain's binary_logloss: 0.0749916\ttrain's f1: 0.968247\tvalid's binary_logloss: 0.295469\tvalid's f1: 0.394057\n",
      "[6600]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[6700]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[6800]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[6900]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7000]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7100]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7200]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7300]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7400]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7500]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7600]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7700]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "[7800]\ttrain's binary_logloss: 0.0744525\ttrain's f1: 0.96868\tvalid's binary_logloss: 0.295416\tvalid's f1: 0.394057\n",
      "Early stopping, best iteration is:\n",
      "[2870]\ttrain's binary_logloss: 0.161581\ttrain's f1: 0.808741\tvalid's binary_logloss: 0.318969\tvalid's f1: 0.427149\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "Training until validation scores don't improve for 5000 rounds\n",
      "[100]\ttrain's binary_logloss: 0.362517\ttrain's f1: 0.422055\tvalid's binary_logloss: 0.376613\tvalid's f1: 0.375994\n",
      "[200]\ttrain's binary_logloss: 0.403983\ttrain's f1: 0.461216\tvalid's binary_logloss: 0.430633\tvalid's f1: 0.382294\n",
      "[300]\ttrain's binary_logloss: 0.41171\ttrain's f1: 0.449735\tvalid's binary_logloss: 0.449299\tvalid's f1: 0.366573\n",
      "[400]\ttrain's binary_logloss: 0.402678\ttrain's f1: 0.464017\tvalid's binary_logloss: 0.450572\tvalid's f1: 0.367309\n",
      "[500]\ttrain's binary_logloss: 0.386905\ttrain's f1: 0.482435\tvalid's binary_logloss: 0.443934\tvalid's f1: 0.371747\n",
      "[600]\ttrain's binary_logloss: 0.370891\ttrain's f1: 0.499189\tvalid's binary_logloss: 0.436086\tvalid's f1: 0.379004\n",
      "[700]\ttrain's binary_logloss: 0.355181\ttrain's f1: 0.515824\tvalid's binary_logloss: 0.427577\tvalid's f1: 0.381741\n",
      "[800]\ttrain's binary_logloss: 0.340697\ttrain's f1: 0.53427\tvalid's binary_logloss: 0.419847\tvalid's f1: 0.385895\n",
      "[900]\ttrain's binary_logloss: 0.326176\ttrain's f1: 0.55352\tvalid's binary_logloss: 0.411952\tvalid's f1: 0.386694\n",
      "[1000]\ttrain's binary_logloss: 0.312233\ttrain's f1: 0.572286\tvalid's binary_logloss: 0.404272\tvalid's f1: 0.390733\n",
      "[1100]\ttrain's binary_logloss: 0.298875\ttrain's f1: 0.592172\tvalid's binary_logloss: 0.396945\tvalid's f1: 0.3959\n",
      "[1200]\ttrain's binary_logloss: 0.286923\ttrain's f1: 0.608774\tvalid's binary_logloss: 0.390785\tvalid's f1: 0.399568\n",
      "[1300]\ttrain's binary_logloss: 0.275362\ttrain's f1: 0.624576\tvalid's binary_logloss: 0.384544\tvalid's f1: 0.39823\n",
      "[1400]\ttrain's binary_logloss: 0.264608\ttrain's f1: 0.637604\tvalid's binary_logloss: 0.378717\tvalid's f1: 0.400605\n",
      "[1500]\ttrain's binary_logloss: 0.254084\ttrain's f1: 0.65303\tvalid's binary_logloss: 0.373182\tvalid's f1: 0.402567\n",
      "[1600]\ttrain's binary_logloss: 0.24437\ttrain's f1: 0.66801\tvalid's binary_logloss: 0.368262\tvalid's f1: 0.403258\n",
      "[1700]\ttrain's binary_logloss: 0.235293\ttrain's f1: 0.682249\tvalid's binary_logloss: 0.363919\tvalid's f1: 0.407\n",
      "[1800]\ttrain's binary_logloss: 0.226948\ttrain's f1: 0.697224\tvalid's binary_logloss: 0.359822\tvalid's f1: 0.406857\n",
      "[1900]\ttrain's binary_logloss: 0.219023\ttrain's f1: 0.707897\tvalid's binary_logloss: 0.355919\tvalid's f1: 0.407689\n",
      "[2000]\ttrain's binary_logloss: 0.211634\ttrain's f1: 0.719672\tvalid's binary_logloss: 0.352415\tvalid's f1: 0.413972\n",
      "[2100]\ttrain's binary_logloss: 0.204452\ttrain's f1: 0.731172\tvalid's binary_logloss: 0.349054\tvalid's f1: 0.411227\n",
      "[2200]\ttrain's binary_logloss: 0.197849\ttrain's f1: 0.742965\tvalid's binary_logloss: 0.346019\tvalid's f1: 0.411199\n",
      "[2300]\ttrain's binary_logloss: 0.191121\ttrain's f1: 0.754618\tvalid's binary_logloss: 0.343075\tvalid's f1: 0.402685\n",
      "[2400]\ttrain's binary_logloss: 0.184958\ttrain's f1: 0.765559\tvalid's binary_logloss: 0.340307\tvalid's f1: 0.400815\n",
      "[2500]\ttrain's binary_logloss: 0.179013\ttrain's f1: 0.777937\tvalid's binary_logloss: 0.337549\tvalid's f1: 0.410896\n",
      "[2600]\ttrain's binary_logloss: 0.17335\ttrain's f1: 0.790146\tvalid's binary_logloss: 0.335167\tvalid's f1: 0.410864\n",
      "[2700]\ttrain's binary_logloss: 0.167777\ttrain's f1: 0.800666\tvalid's binary_logloss: 0.33275\tvalid's f1: 0.414889\n",
      "[2800]\ttrain's binary_logloss: 0.162636\ttrain's f1: 0.811165\tvalid's binary_logloss: 0.33074\tvalid's f1: 0.417266\n",
      "[2900]\ttrain's binary_logloss: 0.157769\ttrain's f1: 0.818835\tvalid's binary_logloss: 0.328505\tvalid's f1: 0.415459\n",
      "[3000]\ttrain's binary_logloss: 0.153112\ttrain's f1: 0.828867\tvalid's binary_logloss: 0.326716\tvalid's f1: 0.416667\n",
      "[3100]\ttrain's binary_logloss: 0.148548\ttrain's f1: 0.837848\tvalid's binary_logloss: 0.324882\tvalid's f1: 0.417889\n",
      "[3200]\ttrain's binary_logloss: 0.144363\ttrain's f1: 0.845703\tvalid's binary_logloss: 0.323366\tvalid's f1: 0.419771\n",
      "[3300]\ttrain's binary_logloss: 0.140349\ttrain's f1: 0.851692\tvalid's binary_logloss: 0.321817\tvalid's f1: 0.414157\n",
      "[3400]\ttrain's binary_logloss: 0.136451\ttrain's f1: 0.86152\tvalid's binary_logloss: 0.320237\tvalid's f1: 0.412186\n",
      "[3500]\ttrain's binary_logloss: 0.132732\ttrain's f1: 0.866693\tvalid's binary_logloss: 0.318863\tvalid's f1: 0.412159\n",
      "[3600]\ttrain's binary_logloss: 0.129219\ttrain's f1: 0.873336\tvalid's binary_logloss: 0.317494\tvalid's f1: 0.410816\n",
      "[3700]\ttrain's binary_logloss: 0.125891\ttrain's f1: 0.879009\tvalid's binary_logloss: 0.316503\tvalid's f1: 0.399579\n",
      "[3800]\ttrain's binary_logloss: 0.122723\ttrain's f1: 0.885843\tvalid's binary_logloss: 0.315374\tvalid's f1: 0.398089\n",
      "[3900]\ttrain's binary_logloss: 0.119621\ttrain's f1: 0.89352\tvalid's binary_logloss: 0.314252\tvalid's f1: 0.394667\n",
      "[4000]\ttrain's binary_logloss: 0.116663\ttrain's f1: 0.899086\tvalid's binary_logloss: 0.313197\tvalid's f1: 0.395664\n",
      "[4100]\ttrain's binary_logloss: 0.113871\ttrain's f1: 0.9051\tvalid's binary_logloss: 0.312221\tvalid's f1: 0.396955\n",
      "[4200]\ttrain's binary_logloss: 0.111293\ttrain's f1: 0.911963\tvalid's binary_logloss: 0.311394\tvalid's f1: 0.396955\n",
      "[4300]\ttrain's binary_logloss: 0.108823\ttrain's f1: 0.917762\tvalid's binary_logloss: 0.310549\tvalid's f1: 0.39879\n",
      "[4400]\ttrain's binary_logloss: 0.106394\ttrain's f1: 0.920493\tvalid's binary_logloss: 0.309928\tvalid's f1: 0.401338\n",
      "[4500]\ttrain's binary_logloss: 0.104073\ttrain's f1: 0.923241\tvalid's binary_logloss: 0.309286\tvalid's f1: 0.401911\n",
      "[4600]\ttrain's binary_logloss: 0.101819\ttrain's f1: 0.927592\tvalid's binary_logloss: 0.308646\tvalid's f1: 0.401811\n",
      "[4700]\ttrain's binary_logloss: 0.0996745\ttrain's f1: 0.931985\tvalid's binary_logloss: 0.308066\tvalid's f1: 0.39818\n",
      "[4800]\ttrain's binary_logloss: 0.0976408\ttrain's f1: 0.93319\tvalid's binary_logloss: 0.307627\tvalid's f1: 0.39886\n",
      "[4900]\ttrain's binary_logloss: 0.095686\ttrain's f1: 0.938855\tvalid's binary_logloss: 0.307143\tvalid's f1: 0.400916\n",
      "[5000]\ttrain's binary_logloss: 0.093858\ttrain's f1: 0.943355\tvalid's binary_logloss: 0.306681\tvalid's f1: 0.397237\n",
      "[5100]\ttrain's binary_logloss: 0.0921059\ttrain's f1: 0.947069\tvalid's binary_logloss: 0.306357\tvalid's f1: 0.398614\n",
      "[5200]\ttrain's binary_logloss: 0.0904342\ttrain's f1: 0.949145\tvalid's binary_logloss: 0.306068\tvalid's f1: 0.4\n",
      "[5300]\ttrain's binary_logloss: 0.088862\ttrain's f1: 0.950395\tvalid's binary_logloss: 0.305694\tvalid's f1: 0.399306\n",
      "[5400]\ttrain's binary_logloss: 0.0873272\ttrain's f1: 0.951648\tvalid's binary_logloss: 0.305463\tvalid's f1: 0.401396\n",
      "[5500]\ttrain's binary_logloss: 0.0857696\ttrain's f1: 0.955007\tvalid's binary_logloss: 0.305282\tvalid's f1: 0.399883\n",
      "[5600]\ttrain's binary_logloss: 0.0842929\ttrain's f1: 0.956695\tvalid's binary_logloss: 0.305001\tvalid's f1: 0.400585\n",
      "[5700]\ttrain's binary_logloss: 0.0829641\ttrain's f1: 0.958389\tvalid's binary_logloss: 0.304821\tvalid's f1: 0.401289\n",
      "[5800]\ttrain's binary_logloss: 0.0816991\ttrain's f1: 0.960941\tvalid's binary_logloss: 0.304675\tvalid's f1: 0.403416\n",
      "[5900]\ttrain's binary_logloss: 0.0804195\ttrain's f1: 0.962222\tvalid's binary_logloss: 0.304405\tvalid's f1: 0.403416\n",
      "[6000]\ttrain's binary_logloss: 0.0792458\ttrain's f1: 0.963078\tvalid's binary_logloss: 0.30432\tvalid's f1: 0.39739\n",
      "[6100]\ttrain's binary_logloss: 0.0780462\ttrain's f1: 0.964795\tvalid's binary_logloss: 0.304121\tvalid's f1: 0.397252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6200]\ttrain's binary_logloss: 0.0769626\ttrain's f1: 0.966518\tvalid's binary_logloss: 0.303937\tvalid's f1: 0.397112\n",
      "[6300]\ttrain's binary_logloss: 0.0759573\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.303763\tvalid's f1: 0.395531\n",
      "[6400]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[6500]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[6600]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[6700]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[6800]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[6900]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7000]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7100]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7200]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7300]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7400]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7500]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7600]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7700]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7800]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[7900]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[8000]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[8100]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "[8200]\ttrain's binary_logloss: 0.0756273\ttrain's f1: 0.96695\tvalid's binary_logloss: 0.30375\tvalid's f1: 0.394816\n",
      "Early stopping, best iteration is:\n",
      "[3259]\ttrain's binary_logloss: 0.142002\ttrain's f1: 0.84902\tvalid's binary_logloss: 0.322515\tvalid's f1: 0.422289\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "Training until validation scores don't improve for 5000 rounds\n",
      "[100]\ttrain's binary_logloss: 0.361274\ttrain's f1: 0.427254\tvalid's binary_logloss: 0.371797\tvalid's f1: 0.38712\n",
      "[200]\ttrain's binary_logloss: 0.404005\ttrain's f1: 0.441509\tvalid's binary_logloss: 0.421564\tvalid's f1: 0.380221\n",
      "[300]\ttrain's binary_logloss: 0.411921\ttrain's f1: 0.451214\tvalid's binary_logloss: 0.43652\tvalid's f1: 0.377265\n",
      "[400]\ttrain's binary_logloss: 0.404742\ttrain's f1: 0.460593\tvalid's binary_logloss: 0.435732\tvalid's f1: 0.379645\n",
      "[500]\ttrain's binary_logloss: 0.391792\ttrain's f1: 0.474179\tvalid's binary_logloss: 0.428979\tvalid's f1: 0.385625\n",
      "[600]\ttrain's binary_logloss: 0.376354\ttrain's f1: 0.491154\tvalid's binary_logloss: 0.419659\tvalid's f1: 0.396798\n",
      "[700]\ttrain's binary_logloss: 0.361622\ttrain's f1: 0.509883\tvalid's binary_logloss: 0.411011\tvalid's f1: 0.393317\n",
      "[800]\ttrain's binary_logloss: 0.346356\ttrain's f1: 0.527832\tvalid's binary_logloss: 0.402067\tvalid's f1: 0.397727\n",
      "[900]\ttrain's binary_logloss: 0.332067\ttrain's f1: 0.544833\tvalid's binary_logloss: 0.393576\tvalid's f1: 0.396364\n",
      "[1000]\ttrain's binary_logloss: 0.319636\ttrain's f1: 0.563636\tvalid's binary_logloss: 0.386656\tvalid's f1: 0.39724\n",
      "[1100]\ttrain's binary_logloss: 0.307889\ttrain's f1: 0.578179\tvalid's binary_logloss: 0.380316\tvalid's f1: 0.398247\n",
      "[1200]\ttrain's binary_logloss: 0.296283\ttrain's f1: 0.595715\tvalid's binary_logloss: 0.374061\tvalid's f1: 0.406662\n",
      "[1300]\ttrain's binary_logloss: 0.285456\ttrain's f1: 0.610277\tvalid's binary_logloss: 0.368056\tvalid's f1: 0.407393\n",
      "[1400]\ttrain's binary_logloss: 0.275177\ttrain's f1: 0.625546\tvalid's binary_logloss: 0.362531\tvalid's f1: 0.410736\n",
      "[1500]\ttrain's binary_logloss: 0.264902\ttrain's f1: 0.642453\tvalid's binary_logloss: 0.356935\tvalid's f1: 0.410172\n",
      "[1600]\ttrain's binary_logloss: 0.255385\ttrain's f1: 0.655746\tvalid's binary_logloss: 0.352001\tvalid's f1: 0.411074\n",
      "[1700]\ttrain's binary_logloss: 0.246197\ttrain's f1: 0.668767\tvalid's binary_logloss: 0.347315\tvalid's f1: 0.416845\n",
      "[1800]\ttrain's binary_logloss: 0.237489\ttrain's f1: 0.680576\tvalid's binary_logloss: 0.34295\tvalid's f1: 0.416305\n",
      "[1900]\ttrain's binary_logloss: 0.229231\ttrain's f1: 0.695578\tvalid's binary_logloss: 0.33892\tvalid's f1: 0.41521\n",
      "[2000]\ttrain's binary_logloss: 0.221221\ttrain's f1: 0.708433\tvalid's binary_logloss: 0.335058\tvalid's f1: 0.413534\n",
      "[2100]\ttrain's binary_logloss: 0.214041\ttrain's f1: 0.719277\tvalid's binary_logloss: 0.331646\tvalid's f1: 0.415735\n",
      "[2200]\ttrain's binary_logloss: 0.206984\ttrain's f1: 0.730704\tvalid's binary_logloss: 0.328213\tvalid's f1: 0.417999\n",
      "[2300]\ttrain's binary_logloss: 0.199983\ttrain's f1: 0.741545\tvalid's binary_logloss: 0.324797\tvalid's f1: 0.423219\n",
      "[2400]\ttrain's binary_logloss: 0.193632\ttrain's f1: 0.751331\tvalid's binary_logloss: 0.321903\tvalid's f1: 0.426829\n",
      "[2500]\ttrain's binary_logloss: 0.187261\ttrain's f1: 0.762184\tvalid's binary_logloss: 0.319077\tvalid's f1: 0.425785\n",
      "[2600]\ttrain's binary_logloss: 0.181378\ttrain's f1: 0.772251\tvalid's binary_logloss: 0.316586\tvalid's f1: 0.42113\n",
      "[2700]\ttrain's binary_logloss: 0.175676\ttrain's f1: 0.783438\tvalid's binary_logloss: 0.314138\tvalid's f1: 0.419339\n",
      "[2800]\ttrain's binary_logloss: 0.170376\ttrain's f1: 0.790885\tvalid's binary_logloss: 0.311806\tvalid's f1: 0.418756\n",
      "[2900]\ttrain's binary_logloss: 0.165158\ttrain's f1: 0.801136\tvalid's binary_logloss: 0.309556\tvalid's f1: 0.424552\n",
      "[3000]\ttrain's binary_logloss: 0.159981\ttrain's f1: 0.812265\tvalid's binary_logloss: 0.30735\tvalid's f1: 0.42268\n",
      "[3100]\ttrain's binary_logloss: 0.155241\ttrain's f1: 0.820584\tvalid's binary_logloss: 0.305451\tvalid's f1: 0.422095\n",
      "[3200]\ttrain's binary_logloss: 0.150447\ttrain's f1: 0.830348\tvalid's binary_logloss: 0.303735\tvalid's f1: 0.42019\n",
      "[3300]\ttrain's binary_logloss: 0.146149\ttrain's f1: 0.838393\tvalid's binary_logloss: 0.30194\tvalid's f1: 0.419562\n",
      "[3400]\ttrain's binary_logloss: 0.141924\ttrain's f1: 0.846264\tvalid's binary_logloss: 0.300325\tvalid's f1: 0.419599\n",
      "[3500]\ttrain's binary_logloss: 0.138012\ttrain's f1: 0.856992\tvalid's binary_logloss: 0.299015\tvalid's f1: 0.421024\n",
      "[3600]\ttrain's binary_logloss: 0.13434\ttrain's f1: 0.864181\tvalid's binary_logloss: 0.297608\tvalid's f1: 0.416204\n",
      "[3700]\ttrain's binary_logloss: 0.130801\ttrain's f1: 0.869041\tvalid's binary_logloss: 0.296296\tvalid's f1: 0.416198\n",
      "[3800]\ttrain's binary_logloss: 0.127361\ttrain's f1: 0.87608\tvalid's binary_logloss: 0.29517\tvalid's f1: 0.418315\n",
      "[3900]\ttrain's binary_logloss: 0.124029\ttrain's f1: 0.884317\tvalid's binary_logloss: 0.293968\tvalid's f1: 0.421203\n",
      "[4000]\ttrain's binary_logloss: 0.120838\ttrain's f1: 0.889772\tvalid's binary_logloss: 0.292857\tvalid's f1: 0.421929\n",
      "[4100]\ttrain's binary_logloss: 0.117871\ttrain's f1: 0.896038\tvalid's binary_logloss: 0.292038\tvalid's f1: 0.420534\n",
      "[4200]\ttrain's binary_logloss: 0.114994\ttrain's f1: 0.903145\tvalid's binary_logloss: 0.29109\tvalid's f1: 0.421267\n",
      "[4300]\ttrain's binary_logloss: 0.112308\ttrain's f1: 0.906551\tvalid's binary_logloss: 0.290268\tvalid's f1: 0.420561\n",
      "[4400]\ttrain's binary_logloss: 0.109759\ttrain's f1: 0.911901\tvalid's binary_logloss: 0.289543\tvalid's f1: 0.42503\n",
      "[4500]\ttrain's binary_logloss: 0.10729\ttrain's f1: 0.916149\tvalid's binary_logloss: 0.288967\tvalid's f1: 0.4273\n",
      "[4600]\ttrain's binary_logloss: 0.10492\ttrain's f1: 0.922399\tvalid's binary_logloss: 0.288329\tvalid's f1: 0.428827\n",
      "[4700]\ttrain's binary_logloss: 0.102578\ttrain's f1: 0.92437\tvalid's binary_logloss: 0.287933\tvalid's f1: 0.429687\n",
      "[4800]\ttrain's binary_logloss: 0.100353\ttrain's f1: 0.929933\tvalid's binary_logloss: 0.287475\tvalid's f1: 0.430464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4900]\ttrain's binary_logloss: 0.0982608\ttrain's f1: 0.931534\tvalid's binary_logloss: 0.287034\tvalid's f1: 0.432024\n",
      "[5000]\ttrain's binary_logloss: 0.0961883\ttrain's f1: 0.935159\tvalid's binary_logloss: 0.286594\tvalid's f1: 0.434517\n",
      "[5100]\ttrain's binary_logloss: 0.0943134\ttrain's f1: 0.93759\tvalid's binary_logloss: 0.286338\tvalid's f1: 0.44\n",
      "[5200]\ttrain's binary_logloss: 0.0924758\ttrain's f1: 0.941672\tvalid's binary_logloss: 0.285812\tvalid's f1: 0.441631\n",
      "[5300]\ttrain's binary_logloss: 0.0908119\ttrain's f1: 0.944962\tvalid's binary_logloss: 0.285535\tvalid's f1: 0.437731\n",
      "[5400]\ttrain's binary_logloss: 0.0891731\ttrain's f1: 0.947445\tvalid's binary_logloss: 0.285332\tvalid's f1: 0.437888\n",
      "[5500]\ttrain's binary_logloss: 0.0875777\ttrain's f1: 0.949525\tvalid's binary_logloss: 0.285075\tvalid's f1: 0.435737\n",
      "[5600]\ttrain's binary_logloss: 0.0859663\ttrain's f1: 0.951194\tvalid's binary_logloss: 0.284767\tvalid's f1: 0.434919\n",
      "[5700]\ttrain's binary_logloss: 0.084527\ttrain's f1: 0.95287\tvalid's binary_logloss: 0.284586\tvalid's f1: 0.432602\n",
      "[5800]\ttrain's binary_logloss: 0.0831585\ttrain's f1: 0.953711\tvalid's binary_logloss: 0.284333\tvalid's f1: 0.431904\n",
      "[5900]\ttrain's binary_logloss: 0.0818579\ttrain's f1: 0.954552\tvalid's binary_logloss: 0.284202\tvalid's f1: 0.434369\n",
      "[6000]\ttrain's binary_logloss: 0.0805179\ttrain's f1: 0.956663\tvalid's binary_logloss: 0.284017\tvalid's f1: 0.439211\n",
      "[6100]\ttrain's binary_logloss: 0.0793801\ttrain's f1: 0.958783\tvalid's binary_logloss: 0.283965\tvalid's f1: 0.438374\n",
      "[6200]\ttrain's binary_logloss: 0.0783157\ttrain's f1: 0.960059\tvalid's binary_logloss: 0.283811\tvalid's f1: 0.436862\n",
      "[6300]\ttrain's binary_logloss: 0.0772411\ttrain's f1: 0.960485\tvalid's binary_logloss: 0.283696\tvalid's f1: 0.437862\n",
      "[6400]\ttrain's binary_logloss: 0.0763452\ttrain's f1: 0.963051\tvalid's binary_logloss: 0.283679\tvalid's f1: 0.437176\n",
      "[6500]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[6600]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[6700]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[6800]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[6900]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7000]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7100]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7200]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7300]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7400]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7500]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7600]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7700]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7800]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[7900]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8000]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8100]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8200]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8300]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8400]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8500]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8600]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8700]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8800]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[8900]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9000]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9100]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9200]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9300]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9400]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9500]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9600]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9700]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9800]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[9900]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "[10000]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6475]\ttrain's binary_logloss: 0.0756985\ttrain's f1: 0.96348\tvalid's binary_logloss: 0.283569\tvalid's f1: 0.437176\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "Training until validation scores don't improve for 5000 rounds\n",
      "[100]\ttrain's binary_logloss: 0.367617\ttrain's f1: 0.410833\tvalid's binary_logloss: 0.369335\tvalid's f1: 0.395137\n",
      "[200]\ttrain's binary_logloss: 0.410963\ttrain's f1: 0.450155\tvalid's binary_logloss: 0.417183\tvalid's f1: 0.40885\n",
      "[300]\ttrain's binary_logloss: 0.419746\ttrain's f1: 0.445625\tvalid's binary_logloss: 0.432733\tvalid's f1: 0.390527\n",
      "[400]\ttrain's binary_logloss: 0.412251\ttrain's f1: 0.451099\tvalid's binary_logloss: 0.432279\tvalid's f1: 0.388802\n",
      "[500]\ttrain's binary_logloss: 0.398642\ttrain's f1: 0.466911\tvalid's binary_logloss: 0.425756\tvalid's f1: 0.390797\n",
      "[600]\ttrain's binary_logloss: 0.382009\ttrain's f1: 0.488978\tvalid's binary_logloss: 0.416612\tvalid's f1: 0.398187\n",
      "[700]\ttrain's binary_logloss: 0.366601\ttrain's f1: 0.50724\tvalid's binary_logloss: 0.40771\tvalid's f1: 0.400726\n",
      "[800]\ttrain's binary_logloss: 0.351152\ttrain's f1: 0.526829\tvalid's binary_logloss: 0.398639\tvalid's f1: 0.40361\n",
      "[900]\ttrain's binary_logloss: 0.336069\ttrain's f1: 0.545652\tvalid's binary_logloss: 0.38991\tvalid's f1: 0.402439\n",
      "[1000]\ttrain's binary_logloss: 0.321316\ttrain's f1: 0.56569\tvalid's binary_logloss: 0.381867\tvalid's f1: 0.40386\n",
      "[1100]\ttrain's binary_logloss: 0.307572\ttrain's f1: 0.58351\tvalid's binary_logloss: 0.374338\tvalid's f1: 0.410884\n",
      "[1200]\ttrain's binary_logloss: 0.29445\ttrain's f1: 0.602387\tvalid's binary_logloss: 0.367666\tvalid's f1: 0.41216\n",
      "[1300]\ttrain's binary_logloss: 0.282663\ttrain's f1: 0.61943\tvalid's binary_logloss: 0.361495\tvalid's f1: 0.419687\n",
      "[1400]\ttrain's binary_logloss: 0.271483\ttrain's f1: 0.635329\tvalid's binary_logloss: 0.355606\tvalid's f1: 0.425117\n",
      "[1500]\ttrain's binary_logloss: 0.261052\ttrain's f1: 0.648474\tvalid's binary_logloss: 0.350177\tvalid's f1: 0.423259\n",
      "[1600]\ttrain's binary_logloss: 0.25127\ttrain's f1: 0.666907\tvalid's binary_logloss: 0.345293\tvalid's f1: 0.42383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1700]\ttrain's binary_logloss: 0.242062\ttrain's f1: 0.681028\tvalid's binary_logloss: 0.340573\tvalid's f1: 0.426009\n",
      "[1800]\ttrain's binary_logloss: 0.233044\ttrain's f1: 0.695489\tvalid's binary_logloss: 0.336152\tvalid's f1: 0.428809\n",
      "[1900]\ttrain's binary_logloss: 0.224547\ttrain's f1: 0.706498\tvalid's binary_logloss: 0.332051\tvalid's f1: 0.429604\n",
      "[2000]\ttrain's binary_logloss: 0.21647\ttrain's f1: 0.71729\tvalid's binary_logloss: 0.328132\tvalid's f1: 0.42654\n",
      "[2100]\ttrain's binary_logloss: 0.209081\ttrain's f1: 0.732866\tvalid's binary_logloss: 0.324343\tvalid's f1: 0.427257\n",
      "[2200]\ttrain's binary_logloss: 0.201905\ttrain's f1: 0.74506\tvalid's binary_logloss: 0.320773\tvalid's f1: 0.431381\n",
      "[2300]\ttrain's binary_logloss: 0.195179\ttrain's f1: 0.755211\tvalid's binary_logloss: 0.317554\tvalid's f1: 0.427662\n",
      "[2400]\ttrain's binary_logloss: 0.188681\ttrain's f1: 0.766234\tvalid's binary_logloss: 0.314709\tvalid's f1: 0.427845\n",
      "[2500]\ttrain's binary_logloss: 0.182397\ttrain's f1: 0.781739\tvalid's binary_logloss: 0.312147\tvalid's f1: 0.432647\n",
      "[2600]\ttrain's binary_logloss: 0.176686\ttrain's f1: 0.791753\tvalid's binary_logloss: 0.309774\tvalid's f1: 0.432829\n",
      "[2700]\ttrain's binary_logloss: 0.171179\ttrain's f1: 0.802622\tvalid's binary_logloss: 0.307588\tvalid's f1: 0.432484\n",
      "[2800]\ttrain's binary_logloss: 0.165762\ttrain's f1: 0.815634\tvalid's binary_logloss: 0.30532\tvalid's f1: 0.429764\n",
      "[2900]\ttrain's binary_logloss: 0.160706\ttrain's f1: 0.824022\tvalid's binary_logloss: 0.303345\tvalid's f1: 0.435\n",
      "[3000]\ttrain's binary_logloss: 0.155927\ttrain's f1: 0.833547\tvalid's binary_logloss: 0.301579\tvalid's f1: 0.433905\n",
      "[3100]\ttrain's binary_logloss: 0.151518\ttrain's f1: 0.842967\tvalid's binary_logloss: 0.299971\tvalid's f1: 0.435336\n",
      "[3200]\ttrain's binary_logloss: 0.147193\ttrain's f1: 0.849254\tvalid's binary_logloss: 0.298471\tvalid's f1: 0.435008\n",
      "[3300]\ttrain's binary_logloss: 0.143137\ttrain's f1: 0.856653\tvalid's binary_logloss: 0.297187\tvalid's f1: 0.433194\n",
      "[3400]\ttrain's binary_logloss: 0.139244\ttrain's f1: 0.865218\tvalid's binary_logloss: 0.296021\tvalid's f1: 0.432131\n",
      "[3500]\ttrain's binary_logloss: 0.135441\ttrain's f1: 0.873251\tvalid's binary_logloss: 0.294681\tvalid's f1: 0.428341\n",
      "[3600]\ttrain's binary_logloss: 0.131762\ttrain's f1: 0.879285\tvalid's binary_logloss: 0.293431\tvalid's f1: 0.424374\n",
      "[3700]\ttrain's binary_logloss: 0.128346\ttrain's f1: 0.889041\tvalid's binary_logloss: 0.292429\tvalid's f1: 0.423077\n",
      "[3800]\ttrain's binary_logloss: 0.125104\ttrain's f1: 0.894186\tvalid's binary_logloss: 0.291651\tvalid's f1: 0.423119\n",
      "[3900]\ttrain's binary_logloss: 0.121975\ttrain's f1: 0.899016\tvalid's binary_logloss: 0.290871\tvalid's f1: 0.425294\n",
      "[4000]\ttrain's binary_logloss: 0.11901\ttrain's f1: 0.905792\tvalid's binary_logloss: 0.290043\tvalid's f1: 0.427447\n",
      "[4100]\ttrain's binary_logloss: 0.116241\ttrain's f1: 0.912671\tvalid's binary_logloss: 0.289512\tvalid's f1: 0.424743\n",
      "[4200]\ttrain's binary_logloss: 0.113469\ttrain's f1: 0.915374\tvalid's binary_logloss: 0.288778\tvalid's f1: 0.426201\n",
      "[4300]\ttrain's binary_logloss: 0.110946\ttrain's f1: 0.918873\tvalid's binary_logloss: 0.288325\tvalid's f1: 0.427005\n",
      "[4400]\ttrain's binary_logloss: 0.108344\ttrain's f1: 0.923186\tvalid's binary_logloss: 0.287499\tvalid's f1: 0.426334\n",
      "[4500]\ttrain's binary_logloss: 0.106034\ttrain's f1: 0.929933\tvalid's binary_logloss: 0.287008\tvalid's f1: 0.427077\n",
      "[4600]\ttrain's binary_logloss: 0.103698\ttrain's f1: 0.933142\tvalid's binary_logloss: 0.286705\tvalid's f1: 0.426402\n",
      "[4700]\ttrain's binary_logloss: 0.101491\ttrain's f1: 0.935968\tvalid's binary_logloss: 0.2862\tvalid's f1: 0.430171\n",
      "[4800]\ttrain's binary_logloss: 0.099321\ttrain's f1: 0.937997\tvalid's binary_logloss: 0.28574\tvalid's f1: 0.427981\n",
      "[4900]\ttrain's binary_logloss: 0.0972975\ttrain's f1: 0.943314\tvalid's binary_logloss: 0.285318\tvalid's f1: 0.4273\n",
      "[5000]\ttrain's binary_logloss: 0.0954277\ttrain's f1: 0.94455\tvalid's binary_logloss: 0.285017\tvalid's f1: 0.427376\n",
      "[5100]\ttrain's binary_logloss: 0.0936375\ttrain's f1: 0.945375\tvalid's binary_logloss: 0.284671\tvalid's f1: 0.430464\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttrain's binary_logloss: 0.385924\ttrain's f1: 0.444471\tvalid's binary_logloss: 0.388489\tvalid's f1: 0.440327\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "Training until validation scores don't improve for 5000 rounds\n",
      "[100]\ttrain's binary_logloss: 0.36328\ttrain's f1: 0.421902\tvalid's binary_logloss: 0.378748\tvalid's f1: 0.376063\n",
      "[200]\ttrain's binary_logloss: 0.406657\ttrain's f1: 0.448729\tvalid's binary_logloss: 0.432676\tvalid's f1: 0.374449\n",
      "[300]\ttrain's binary_logloss: 0.414878\ttrain's f1: 0.445391\tvalid's binary_logloss: 0.45057\tvalid's f1: 0.36084\n",
      "[400]\ttrain's binary_logloss: 0.408118\ttrain's f1: 0.45528\tvalid's binary_logloss: 0.451254\tvalid's f1: 0.358321\n",
      "[500]\ttrain's binary_logloss: 0.395227\ttrain's f1: 0.469212\tvalid's binary_logloss: 0.444953\tvalid's f1: 0.366564\n",
      "[600]\ttrain's binary_logloss: 0.379903\ttrain's f1: 0.48675\tvalid's binary_logloss: 0.436094\tvalid's f1: 0.369085\n",
      "[700]\ttrain's binary_logloss: 0.365279\ttrain's f1: 0.502359\tvalid's binary_logloss: 0.4275\tvalid's f1: 0.365304\n",
      "[800]\ttrain's binary_logloss: 0.35058\ttrain's f1: 0.518503\tvalid's binary_logloss: 0.418888\tvalid's f1: 0.371984\n",
      "[900]\ttrain's binary_logloss: 0.336404\ttrain's f1: 0.53785\tvalid's binary_logloss: 0.410625\tvalid's f1: 0.37183\n",
      "[1000]\ttrain's binary_logloss: 0.32209\ttrain's f1: 0.557857\tvalid's binary_logloss: 0.402411\tvalid's f1: 0.378122\n",
      "[1100]\ttrain's binary_logloss: 0.308847\ttrain's f1: 0.574505\tvalid's binary_logloss: 0.394778\tvalid's f1: 0.385876\n",
      "[1200]\ttrain's binary_logloss: 0.296309\ttrain's f1: 0.589916\tvalid's binary_logloss: 0.38768\tvalid's f1: 0.393175\n",
      "[1300]\ttrain's binary_logloss: 0.284343\ttrain's f1: 0.606691\tvalid's binary_logloss: 0.381163\tvalid's f1: 0.399618\n",
      "[1400]\ttrain's binary_logloss: 0.273311\ttrain's f1: 0.62204\tvalid's binary_logloss: 0.374944\tvalid's f1: 0.402724\n",
      "[1500]\ttrain's binary_logloss: 0.262567\ttrain's f1: 0.638635\tvalid's binary_logloss: 0.369327\tvalid's f1: 0.40938\n",
      "[1600]\ttrain's binary_logloss: 0.2523\ttrain's f1: 0.656021\tvalid's binary_logloss: 0.364044\tvalid's f1: 0.406718\n",
      "[1700]\ttrain's binary_logloss: 0.242168\ttrain's f1: 0.675379\tvalid's binary_logloss: 0.35879\tvalid's f1: 0.411692\n",
      "[1800]\ttrain's binary_logloss: 0.233038\ttrain's f1: 0.692234\tvalid's binary_logloss: 0.354021\tvalid's f1: 0.412134\n",
      "[1900]\ttrain's binary_logloss: 0.224647\ttrain's f1: 0.705345\tvalid's binary_logloss: 0.349879\tvalid's f1: 0.410432\n",
      "[2000]\ttrain's binary_logloss: 0.216588\ttrain's f1: 0.719516\tvalid's binary_logloss: 0.345927\tvalid's f1: 0.412527\n",
      "[2100]\ttrain's binary_logloss: 0.208514\ttrain's f1: 0.734678\tvalid's binary_logloss: 0.341914\tvalid's f1: 0.410777\n",
      "[2200]\ttrain's binary_logloss: 0.201073\ttrain's f1: 0.749711\tvalid's binary_logloss: 0.33846\tvalid's f1: 0.411236\n",
      "[2300]\ttrain's binary_logloss: 0.194006\ttrain's f1: 0.763211\tvalid's binary_logloss: 0.335249\tvalid's f1: 0.410584\n",
      "[2400]\ttrain's binary_logloss: 0.187427\ttrain's f1: 0.773909\tvalid's binary_logloss: 0.332305\tvalid's f1: 0.407545\n",
      "[2500]\ttrain's binary_logloss: 0.181187\ttrain's f1: 0.784859\tvalid's binary_logloss: 0.329463\tvalid's f1: 0.410342\n",
      "[2600]\ttrain's binary_logloss: 0.175301\ttrain's f1: 0.797885\tvalid's binary_logloss: 0.326851\tvalid's f1: 0.412075\n",
      "[2700]\ttrain's binary_logloss: 0.169652\ttrain's f1: 0.805311\tvalid's binary_logloss: 0.32423\tvalid's f1: 0.413223\n",
      "[2800]\ttrain's binary_logloss: 0.164282\ttrain's f1: 0.818102\tvalid's binary_logloss: 0.321972\tvalid's f1: 0.410688\n",
      "[2900]\ttrain's binary_logloss: 0.159223\ttrain's f1: 0.826857\tvalid's binary_logloss: 0.319952\tvalid's f1: 0.410616\n",
      "[3000]\ttrain's binary_logloss: 0.154331\ttrain's f1: 0.832906\tvalid's binary_logloss: 0.318051\tvalid's f1: 0.412449\n",
      "[3100]\ttrain's binary_logloss: 0.149796\ttrain's f1: 0.839695\tvalid's binary_logloss: 0.316263\tvalid's f1: 0.414337\n",
      "[3200]\ttrain's binary_logloss: 0.145363\ttrain's f1: 0.846927\tvalid's binary_logloss: 0.31471\tvalid's f1: 0.41301\n",
      "[3300]\ttrain's binary_logloss: 0.141258\ttrain's f1: 0.85496\tvalid's binary_logloss: 0.31342\tvalid's f1: 0.414278\n",
      "[3400]\ttrain's binary_logloss: 0.137331\ttrain's f1: 0.861771\tvalid's binary_logloss: 0.311772\tvalid's f1: 0.415571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3500]\ttrain's binary_logloss: 0.133497\ttrain's f1: 0.871492\tvalid's binary_logloss: 0.310461\tvalid's f1: 0.414894\n",
      "[3600]\ttrain's binary_logloss: 0.130148\ttrain's f1: 0.878214\tvalid's binary_logloss: 0.309449\tvalid's f1: 0.416889\n",
      "[3700]\ttrain's binary_logloss: 0.126689\ttrain's f1: 0.883955\tvalid's binary_logloss: 0.3085\tvalid's f1: 0.411478\n",
      "[3800]\ttrain's binary_logloss: 0.123348\ttrain's f1: 0.894555\tvalid's binary_logloss: 0.307358\tvalid's f1: 0.410773\n",
      "[3900]\ttrain's binary_logloss: 0.12018\ttrain's f1: 0.899016\tvalid's binary_logloss: 0.306364\tvalid's f1: 0.415512\n",
      "[4000]\ttrain's binary_logloss: 0.11724\ttrain's f1: 0.906171\tvalid's binary_logloss: 0.305536\tvalid's f1: 0.415505\n",
      "[4100]\ttrain's binary_logloss: 0.114438\ttrain's f1: 0.912286\tvalid's binary_logloss: 0.304736\tvalid's f1: 0.414102\n",
      "[4200]\ttrain's binary_logloss: 0.111664\ttrain's f1: 0.915761\tvalid's binary_logloss: 0.303908\tvalid's f1: 0.411964\n",
      "[4300]\ttrain's binary_logloss: 0.10905\ttrain's f1: 0.919654\tvalid's binary_logloss: 0.303167\tvalid's f1: 0.412635\n",
      "[4400]\ttrain's binary_logloss: 0.106522\ttrain's f1: 0.923975\tvalid's binary_logloss: 0.302348\tvalid's f1: 0.409742\n",
      "[4500]\ttrain's binary_logloss: 0.104103\ttrain's f1: 0.92754\tvalid's binary_logloss: 0.301735\tvalid's f1: 0.413271\n",
      "[4600]\ttrain's binary_logloss: 0.101756\ttrain's f1: 0.930733\tvalid's binary_logloss: 0.301109\tvalid's f1: 0.412522\n",
      "[4700]\ttrain's binary_logloss: 0.0995835\ttrain's f1: 0.932337\tvalid's binary_logloss: 0.300594\tvalid's f1: 0.412522\n",
      "[4800]\ttrain's binary_logloss: 0.0975039\ttrain's f1: 0.934351\tvalid's binary_logloss: 0.300143\tvalid's f1: 0.41173\n",
      "[4900]\ttrain's binary_logloss: 0.0954338\ttrain's f1: 0.938811\tvalid's binary_logloss: 0.299632\tvalid's f1: 0.414678\n",
      "[5000]\ttrain's binary_logloss: 0.093527\ttrain's f1: 0.942492\tvalid's binary_logloss: 0.299187\tvalid's f1: 0.413174\n",
      "[5100]\ttrain's binary_logloss: 0.0917263\ttrain's f1: 0.94455\tvalid's binary_logloss: 0.298864\tvalid's f1: 0.412402\n",
      "[5200]\ttrain's binary_logloss: 0.0900291\ttrain's f1: 0.94786\tvalid's binary_logloss: 0.298535\tvalid's f1: 0.411622\n",
      "[5300]\ttrain's binary_logloss: 0.088383\ttrain's f1: 0.949108\tvalid's binary_logloss: 0.298104\tvalid's f1: 0.413877\n",
      "[5400]\ttrain's binary_logloss: 0.0868331\ttrain's f1: 0.951194\tvalid's binary_logloss: 0.297852\tvalid's f1: 0.410791\n",
      "[5500]\ttrain's binary_logloss: 0.0853097\ttrain's f1: 0.952451\tvalid's binary_logloss: 0.297664\tvalid's f1: 0.410037\n",
      "[5600]\ttrain's binary_logloss: 0.0838679\ttrain's f1: 0.954131\tvalid's binary_logloss: 0.297505\tvalid's f1: 0.410037\n",
      "[5700]\ttrain's binary_logloss: 0.0825392\ttrain's f1: 0.954552\tvalid's binary_logloss: 0.297404\tvalid's f1: 0.411548\n",
      "[5800]\ttrain's binary_logloss: 0.0811813\ttrain's f1: 0.957934\tvalid's binary_logloss: 0.297229\tvalid's f1: 0.41307\n",
      "[5900]\ttrain's binary_logloss: 0.0799043\ttrain's f1: 0.959633\tvalid's binary_logloss: 0.297101\tvalid's f1: 0.405237\n",
      "[6000]\ttrain's binary_logloss: 0.0786727\ttrain's f1: 0.960485\tvalid's binary_logloss: 0.296922\tvalid's f1: 0.401369\n",
      "[6100]\ttrain's binary_logloss: 0.0775787\ttrain's f1: 0.962194\tvalid's binary_logloss: 0.296769\tvalid's f1: 0.399002\n",
      "[6200]\ttrain's binary_logloss: 0.0765082\ttrain's f1: 0.964339\tvalid's binary_logloss: 0.296651\tvalid's f1: 0.40201\n",
      "[6300]\ttrain's binary_logloss: 0.0755837\ttrain's f1: 0.966061\tvalid's binary_logloss: 0.296598\tvalid's f1: 0.403531\n",
      "[6400]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[6500]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[6600]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[6700]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[6800]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[6900]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7000]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7100]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7200]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7300]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7400]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7500]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7600]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7700]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7800]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[7900]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8000]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8100]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8200]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8300]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8400]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8500]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8600]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8700]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "[8800]\ttrain's binary_logloss: 0.0752449\ttrain's f1: 0.965199\tvalid's binary_logloss: 0.296561\tvalid's f1: 0.401137\n",
      "Early stopping, best iteration is:\n",
      "[3892]\ttrain's binary_logloss: 0.120422\ttrain's f1: 0.899016\tvalid's binary_logloss: 0.306432\tvalid's f1: 0.417588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "def custom_f1_eval(y_true, y_pred):\n",
    "    yr = y_pred\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] >= 0.5:\n",
    "            yr[i] = 1\n",
    "        else :\n",
    "            yr[i] = 0\n",
    "    precision = precision_score(y_true, yr)\n",
    "    recall = recall_score(y_true, yr)\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = (precision * recall) / (0.4 * precision + 0.6 * recall)\n",
    "    return \"f1\", f1, True\n",
    "model = lgb.LGBMClassifier(objective='binary',\n",
    "                           boosting_type='gbdt',\n",
    "                           num_leaves=32,\n",
    "                           max_depth=5,\n",
    "                           learning_rate=0.01,\n",
    "                           n_estimators=10000,\n",
    "                           subsample=0.8,\n",
    "                           feature_fraction=0.6,\n",
    "                           reg_alpha=10,\n",
    "                           reg_lambda=12,\n",
    "                           random_state=seed,\n",
    "                           metric = None,\n",
    "                           is_unbalance=True)\n",
    "df_oof = df_train[['id',ycol]].copy()\n",
    "df_oof['prob'] = 0\n",
    "prediction = df_test[['id']]\n",
    "prediction['prob'] = 0\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        kfold.split(df_train[feature_names], df_train[ycol])):\n",
    "    X_train = df_train.iloc[trn_idx][feature_names]\n",
    "    Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "    X_val = df_train.iloc[val_idx][feature_names]\n",
    "    Y_val = df_train.iloc[val_idx][ycol]\n",
    "    print('\\nFold_{} Training ================================\\n'.format(\n",
    "        fold_id + 1))\n",
    "\n",
    "    lgb_model = model.fit(X_train,\n",
    "                          Y_train,\n",
    "                          eval_names=['train', 'valid'],\n",
    "                          eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                          eval_metric=lambda y_true, y_pred: [custom_f1_eval(y_true, y_pred)],\n",
    "                          verbose=100,\n",
    "                          early_stopping_rounds=5000)\n",
    "\n",
    "    pred_val = lgb_model.predict_proba(\n",
    "        X_val, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "\n",
    "    df_oof.loc[val_idx, 'prob'] = pred_val\n",
    "\n",
    "    pred_test = lgb_model.predict_proba(\n",
    "        df_test[feature_names], num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    \n",
    "    prediction['prob'] += pred_test / kfold.n_splits\n",
    "\n",
    "    del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T06:23:58.624510Z",
     "start_time": "2020-11-04T06:23:57.769881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.38563243719700313\n"
     ]
    }
   ],
   "source": [
    "    yr = df_train['id'].copy()\n",
    "    for i in range(len(yr)):\n",
    "        if df_oof['prob'][i] >= 0.6:\n",
    "            yr[i] = 1\n",
    "        else :\n",
    "            yr[i] = 0\n",
    "    precision = precision_score(df_oof[ycol], yr)\n",
    "    recall = recall_score(df_oof[ycol],yr)\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = (precision * recall) / (0.4 * precision + 0.6 * recall)\n",
    "print('f1:', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:58:17.896815Z",
     "start_time": "2020-11-03T05:58:17.823794Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('sub', exist_ok=True)\n",
    "prediction.to_csv('sub/xinwangyinhang_{}.csv'.format(auc), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T03:03:48.209748Z",
     "start_time": "2020-11-04T03:03:48.118230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  target  prob\n",
       "0    0     0.0   0.0\n",
       "1    1     0.0   0.0\n",
       "2    2     1.0   0.0\n",
       "3    3     0.0   0.0\n",
       "4    4     1.0   0.0\n",
       "5    5     1.0   0.0\n",
       "6    6     1.0   0.0\n",
       "7    7     0.0   0.0\n",
       "8    8     0.0   0.0\n",
       "9    9     0.0   0.0\n",
       "10  10     1.0   0.0\n",
       "11  11     1.0   0.0\n",
       "12  12     0.0   0.0\n",
       "13  13     1.0   0.0\n",
       "14  14     1.0   0.0\n",
       "15  15     1.0   0.0\n",
       "16  16     1.0   0.0\n",
       "17  17     0.0   0.0\n",
       "18  18     0.0   0.0\n",
       "19  19     0.0   0.0\n",
       "20  20     0.0   0.0\n",
       "21  21     0.0   0.0\n",
       "22  22     0.0   0.0\n",
       "23  23     0.0   0.0\n",
       "24  24     0.0   0.0\n",
       "25  25     0.0   0.0\n",
       "26  26     0.0   0.0\n",
       "27  27     0.0   0.0\n",
       "28  28     0.0   0.0\n",
       "29  29     0.0   0.0\n",
       "30  30     0.0   0.0\n",
       "31  31     0.0   0.0\n",
       "32  32     0.0   0.0\n",
       "33  33     0.0   0.0\n",
       "34  34     0.0   0.0\n",
       "35  35     0.0   0.0\n",
       "36  36     0.0   0.0\n",
       "37  37     0.0   0.0\n",
       "38  38     0.0   0.0\n",
       "39  39     1.0   0.0\n",
       "40  40     1.0   0.0\n",
       "41  41     1.0   0.0\n",
       "42  42     0.0   0.0\n",
       "43  43     0.0   0.0\n",
       "44  44     0.0   0.0\n",
       "45  45     0.0   0.0\n",
       "46  46     0.0   0.0\n",
       "47  47     0.0   0.0\n",
       "48  48     0.0   0.0\n",
       "49  49     0.0   0.0\n",
       "50  50     0.0   0.0\n",
       "51  51     0.0   0.0\n",
       "52  52     0.0   0.0\n",
       "53  53     0.0   0.0\n",
       "54  54     1.0   0.0\n",
       "55  55     0.0   0.0\n",
       "56  56     0.0   0.0\n",
       "57  57     1.0   0.0\n",
       "58  58     0.0   0.0\n",
       "59  59     0.0   0.0\n",
       "60  60     0.0   0.0\n",
       "61  61     0.0   0.0\n",
       "62  62     0.0   0.0\n",
       "63  63     0.0   0.0\n",
       "64  64     1.0   0.0\n",
       "65  65     0.0   0.0\n",
       "66  66     0.0   0.0\n",
       "67  67     0.0   0.0\n",
       "68  68     0.0   0.0\n",
       "69  69     1.0   0.0\n",
       "70  70     0.0   0.0\n",
       "71  71     1.0   0.0\n",
       "72  72     0.0   0.0\n",
       "73  73     0.0   0.0\n",
       "74  74     0.0   0.0\n",
       "75  75     0.0   0.0\n",
       "76  76     0.0   0.0\n",
       "77  77     1.0   0.0\n",
       "78  78     0.0   0.0\n",
       "79  79     0.0   0.0\n",
       "80  80     1.0   0.0\n",
       "81  81     0.0   0.0\n",
       "82  82     0.0   0.0\n",
       "83  83     0.0   0.0\n",
       "84  84     0.0   0.0\n",
       "85  85     0.0   0.0\n",
       "86  86     0.0   0.0\n",
       "87  87     0.0   0.0\n",
       "88  88     0.0   0.0\n",
       "89  89     0.0   0.0\n",
       "90  90     0.0   0.0\n",
       "91  91     0.0   0.0\n",
       "92  92     0.0   0.0\n",
       "93  93     0.0   0.0\n",
       "94  94     0.0   0.0\n",
       "95  95     1.0   0.0\n",
       "96  96     0.0   0.0\n",
       "97  97     1.0   0.0\n",
       "98  98     1.0   0.0\n",
       "99  99     0.0   0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oof.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T02:49:32.353660Z",
     "start_time": "2020-11-04T02:49:31.942628Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('prob', exist_ok=True)\n",
    "\n",
    "prediction.to_csv('prob/sub_lgb{}.csv'.format(v), index=False)\n",
    "df_oof[['id', 'prob', ycol]].to_csv('prob/oof_lgb{}.csv'.format(v), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T05:58:42.389387Z",
     "start_time": "2020-11-03T05:58:42.336402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.709167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.749517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.867418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.719255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.671743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.847238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.521465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.746615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.804653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.845241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.615163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.482939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.387728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.223648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.430053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.199354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  target      prob\n",
       "0    0     0.0  0.194634\n",
       "1    1     0.0  0.003817\n",
       "2    2     1.0  0.388696\n",
       "3    3     0.0  0.006898\n",
       "4    4     1.0  0.889966\n",
       "5    5     1.0  0.698051\n",
       "6    6     1.0  0.709167\n",
       "7    7     0.0  0.267751\n",
       "8    8     0.0  0.427973\n",
       "9    9     0.0  0.644526\n",
       "10  10     1.0  0.749517\n",
       "11  11     1.0  0.286833\n",
       "12  12     0.0  0.373983\n",
       "13  13     1.0  0.867418\n",
       "14  14     1.0  0.943238\n",
       "15  15     1.0  0.826721\n",
       "16  16     1.0  0.504709\n",
       "17  17     0.0  0.115914\n",
       "18  18     0.0  0.692117\n",
       "19  19     0.0  0.719255\n",
       "20  20     0.0  0.708916\n",
       "21  21     0.0  0.501636\n",
       "22  22     0.0  0.717309\n",
       "23  23     0.0  0.304437\n",
       "24  24     0.0  0.355149\n",
       "25  25     0.0  0.449889\n",
       "26  26     0.0  0.668481\n",
       "27  27     0.0  0.007354\n",
       "28  28     0.0  0.349206\n",
       "29  29     0.0  0.036912\n",
       "30  30     0.0  0.370096\n",
       "31  31     0.0  0.174533\n",
       "32  32     0.0  0.321527\n",
       "33  33     0.0  0.024648\n",
       "34  34     0.0  0.171397\n",
       "35  35     0.0  0.486193\n",
       "36  36     0.0  0.671743\n",
       "37  37     0.0  0.333353\n",
       "38  38     0.0  0.051546\n",
       "39  39     1.0  0.677388\n",
       "40  40     1.0  0.847238\n",
       "41  41     1.0  0.521465\n",
       "42  42     0.0  0.785521\n",
       "43  43     0.0  0.377442\n",
       "44  44     0.0  0.253662\n",
       "45  45     0.0  0.352999\n",
       "46  46     0.0  0.028918\n",
       "47  47     0.0  0.137679\n",
       "48  48     0.0  0.179935\n",
       "49  49     0.0  0.399023\n",
       "50  50     0.0  0.004829\n",
       "51  51     0.0  0.166196\n",
       "52  52     0.0  0.031556\n",
       "53  53     0.0  0.045497\n",
       "54  54     1.0  0.746615\n",
       "55  55     0.0  0.217906\n",
       "56  56     0.0  0.326978\n",
       "57  57     1.0  0.804653\n",
       "58  58     0.0  0.017680\n",
       "59  59     0.0  0.022724\n",
       "60  60     0.0  0.650944\n",
       "61  61     0.0  0.188608\n",
       "62  62     0.0  0.014813\n",
       "63  63     0.0  0.009379\n",
       "64  64     1.0  0.845241\n",
       "65  65     0.0  0.048540\n",
       "66  66     0.0  0.645175\n",
       "67  67     0.0  0.132873\n",
       "68  68     0.0  0.063758\n",
       "69  69     1.0  0.615163\n",
       "70  70     0.0  0.054646\n",
       "71  71     1.0  0.902985\n",
       "72  72     0.0  0.229229\n",
       "73  73     0.0  0.381167\n",
       "74  74     0.0  0.096297\n",
       "75  75     0.0  0.141001\n",
       "76  76     0.0  0.158371\n",
       "77  77     1.0  0.482939\n",
       "78  78     0.0  0.342633\n",
       "79  79     0.0  0.015576\n",
       "80  80     1.0  0.387728\n",
       "81  81     0.0  0.223648\n",
       "82  82     0.0  0.126234\n",
       "83  83     0.0  0.132139\n",
       "84  84     0.0  0.293049\n",
       "85  85     0.0  0.015335\n",
       "86  86     0.0  0.005998\n",
       "87  87     0.0  0.283975\n",
       "88  88     0.0  0.440259\n",
       "89  89     0.0  0.644787\n",
       "90  90     0.0  0.383294\n",
       "91  91     0.0  0.430053\n",
       "92  92     0.0  0.495358\n",
       "93  93     0.0  0.039412\n",
       "94  94     0.0  0.514923\n",
       "95  95     1.0  0.596845\n",
       "96  96     0.0  0.532195\n",
       "97  97     1.0  0.103285\n",
       "98  98     1.0  0.199354\n",
       "99  99     0.0  0.194263"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oof.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T06:44:34.577340Z",
     "start_time": "2020-11-03T06:30:46.724097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "[0]\tvalidation_0-auc:0.70505\tvalidation_1-auc:0.67701\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-auc:0.82503\tvalidation_1-auc:0.77034\n",
      "[200]\tvalidation_0-auc:0.85697\tvalidation_1-auc:0.79114\n",
      "[300]\tvalidation_0-auc:0.87672\tvalidation_1-auc:0.79920\n",
      "[400]\tvalidation_0-auc:0.89124\tvalidation_1-auc:0.80453\n",
      "[500]\tvalidation_0-auc:0.90293\tvalidation_1-auc:0.80885\n",
      "[600]\tvalidation_0-auc:0.91332\tvalidation_1-auc:0.81121\n",
      "[700]\tvalidation_0-auc:0.92305\tvalidation_1-auc:0.81369\n",
      "[800]\tvalidation_0-auc:0.93143\tvalidation_1-auc:0.81527\n",
      "[900]\tvalidation_0-auc:0.93933\tvalidation_1-auc:0.81655\n",
      "[1000]\tvalidation_0-auc:0.94638\tvalidation_1-auc:0.81817\n",
      "[1100]\tvalidation_0-auc:0.95249\tvalidation_1-auc:0.81946\n",
      "[1200]\tvalidation_0-auc:0.95786\tvalidation_1-auc:0.82009\n",
      "[1300]\tvalidation_0-auc:0.96254\tvalidation_1-auc:0.82054\n",
      "[1400]\tvalidation_0-auc:0.96675\tvalidation_1-auc:0.82065\n",
      "Stopping. Best iteration:\n",
      "[1355]\tvalidation_0-auc:0.96485\tvalidation_1-auc:0.82078\n",
      "\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "[0]\tvalidation_0-auc:0.70358\tvalidation_1-auc:0.68448\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-auc:0.82120\tvalidation_1-auc:0.77413\n",
      "[200]\tvalidation_0-auc:0.85273\tvalidation_1-auc:0.79429\n",
      "[300]\tvalidation_0-auc:0.87399\tvalidation_1-auc:0.80810\n",
      "[400]\tvalidation_0-auc:0.88935\tvalidation_1-auc:0.81671\n",
      "[500]\tvalidation_0-auc:0.90233\tvalidation_1-auc:0.82288\n",
      "[600]\tvalidation_0-auc:0.91367\tvalidation_1-auc:0.82607\n",
      "[700]\tvalidation_0-auc:0.92413\tvalidation_1-auc:0.82841\n",
      "[800]\tvalidation_0-auc:0.93311\tvalidation_1-auc:0.82983\n",
      "Stopping. Best iteration:\n",
      "[809]\tvalidation_0-auc:0.93392\tvalidation_1-auc:0.83000\n",
      "\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "[0]\tvalidation_0-auc:0.70550\tvalidation_1-auc:0.69927\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-auc:0.82307\tvalidation_1-auc:0.78315\n",
      "[200]\tvalidation_0-auc:0.85280\tvalidation_1-auc:0.79981\n",
      "[300]\tvalidation_0-auc:0.87116\tvalidation_1-auc:0.81296\n",
      "[400]\tvalidation_0-auc:0.88696\tvalidation_1-auc:0.82342\n",
      "[500]\tvalidation_0-auc:0.89951\tvalidation_1-auc:0.83021\n",
      "[600]\tvalidation_0-auc:0.91075\tvalidation_1-auc:0.83449\n",
      "[700]\tvalidation_0-auc:0.92098\tvalidation_1-auc:0.83755\n",
      "[800]\tvalidation_0-auc:0.93007\tvalidation_1-auc:0.83940\n",
      "[900]\tvalidation_0-auc:0.93800\tvalidation_1-auc:0.84127\n",
      "[1000]\tvalidation_0-auc:0.94530\tvalidation_1-auc:0.84262\n",
      "[1100]\tvalidation_0-auc:0.95152\tvalidation_1-auc:0.84381\n",
      "[1200]\tvalidation_0-auc:0.95699\tvalidation_1-auc:0.84395\n",
      "Stopping. Best iteration:\n",
      "[1248]\tvalidation_0-auc:0.95942\tvalidation_1-auc:0.84419\n",
      "\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "[0]\tvalidation_0-auc:0.69761\tvalidation_1-auc:0.71267\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-auc:0.81316\tvalidation_1-auc:0.80657\n",
      "[200]\tvalidation_0-auc:0.84773\tvalidation_1-auc:0.82501\n",
      "[300]\tvalidation_0-auc:0.86892\tvalidation_1-auc:0.83575\n",
      "[400]\tvalidation_0-auc:0.88487\tvalidation_1-auc:0.84209\n",
      "[500]\tvalidation_0-auc:0.89697\tvalidation_1-auc:0.84657\n",
      "[600]\tvalidation_0-auc:0.90898\tvalidation_1-auc:0.84916\n",
      "[700]\tvalidation_0-auc:0.91950\tvalidation_1-auc:0.85140\n",
      "[800]\tvalidation_0-auc:0.92901\tvalidation_1-auc:0.85307\n",
      "Stopping. Best iteration:\n",
      "[836]\tvalidation_0-auc:0.93184\tvalidation_1-auc:0.85320\n",
      "\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n",
      "[0]\tvalidation_0-auc:0.70024\tvalidation_1-auc:0.68413\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 50 rounds.\n",
      "[100]\tvalidation_0-auc:0.82202\tvalidation_1-auc:0.77666\n",
      "[200]\tvalidation_0-auc:0.85190\tvalidation_1-auc:0.80004\n",
      "[300]\tvalidation_0-auc:0.87223\tvalidation_1-auc:0.81472\n",
      "[400]\tvalidation_0-auc:0.88778\tvalidation_1-auc:0.82469\n",
      "[500]\tvalidation_0-auc:0.90006\tvalidation_1-auc:0.83146\n",
      "[600]\tvalidation_0-auc:0.91086\tvalidation_1-auc:0.83614\n",
      "[700]\tvalidation_0-auc:0.92105\tvalidation_1-auc:0.83927\n",
      "[800]\tvalidation_0-auc:0.93018\tvalidation_1-auc:0.84127\n",
      "[900]\tvalidation_0-auc:0.93824\tvalidation_1-auc:0.84242\n",
      "[1000]\tvalidation_0-auc:0.94574\tvalidation_1-auc:0.84349\n",
      "[1100]\tvalidation_0-auc:0.95192\tvalidation_1-auc:0.84442\n",
      "[1200]\tvalidation_0-auc:0.95762\tvalidation_1-auc:0.84503\n",
      "[1300]\tvalidation_0-auc:0.96263\tvalidation_1-auc:0.84546\n",
      "Stopping. Best iteration:\n",
      "[1330]\tvalidation_0-auc:0.96388\tvalidation_1-auc:0.84568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier( \n",
    "                           max_depth=6,\n",
    "                           learning_rate=0.01,\n",
    "                           n_estimators=10000,\n",
    "                           subsample=0.8,\n",
    "                           reg_alpha=10,\n",
    "                           reg_lambda=12,\n",
    "#                             tree_method='gpu_hist',\n",
    "                           random_state=seed)\n",
    "\n",
    "df_oof = df_train[['id', ycol]].copy()\n",
    "df_oof['prob'] = 0\n",
    "prediction = df_test[['id']]\n",
    "prediction['prob'] = 0\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold_id, (trn_idx, val_idx) in enumerate(\n",
    "        kfold.split(df_train[feature_names], df_train[ycol])):\n",
    "    X_train = df_train.iloc[trn_idx][feature_names]\n",
    "    Y_train = df_train.iloc[trn_idx][ycol]\n",
    "\n",
    "    X_val = df_train.iloc[val_idx][feature_names]\n",
    "    Y_val = df_train.iloc[val_idx][ycol]\n",
    "\n",
    "    print('\\nFold_{} Training ================================\\n'.format(\n",
    "        fold_id + 1))\n",
    "\n",
    "    lgb_model = model.fit(X_train,\n",
    "                          Y_train,\n",
    "                          eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                          verbose=100,\n",
    "                          eval_metric='auc', \n",
    "                          early_stopping_rounds=50)\n",
    "\n",
    "    pred_val = lgb_model.predict_proba(X_val,)[:, 1]\n",
    "    df_oof.loc[val_idx, 'prob'] = pred_val\n",
    "\n",
    "    pred_test = lgb_model.predict_proba(df_test[feature_names])[:, 1]\n",
    "    prediction['prob'] += pred_test / kfold.n_splits\n",
    "\n",
    "    del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T13:33:38.235738Z",
     "start_time": "2020-11-03T13:33:37.667696Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('prob', exist_ok=True)\n",
    "\n",
    "prediction.to_csv('prob/sub_xgb{}.csv'.format(v), index=False)\n",
    "df_oof[['id', 'prob', ycol]].to_csv('prob/oof_xgb{}.csv'.format(v), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
